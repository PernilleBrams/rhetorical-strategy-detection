# ğŸ›ï¸ Rhetorical-Strategy-Detection
 
## ğŸ“š Project Overview

This repository contains all code, notebooks, and data pipelines developed for my masterâ€™s thesis in Cognitive Science, titled **DISARM: Deception Identification and Statement Assessment in Rhetorical Messaging Using NLP**. The goal of this project is to build an end-to-end NLP pipeline for detecting rhetorical strategiesâ€”particularly distinguishing â€œAnswersâ€ from â€œNon-Answersâ€â€”in Danish parliamentary question-answer exchanges (QAEs).

---

## ğŸ“ Folder Structure

Below is a high-level overview of the directory structure:

```
rhetorical-strategy-detection
â”‚
â”œâ”€â”€ datasets
â”‚   â”œâ”€â”€ finetuning_and_test_splits
â”‚   â”‚   â”œâ”€â”€ df_train_ANON.csv
â”‚   â”‚   â”œâ”€â”€ df_val_ANON.csv
â”‚   â”‚   â””â”€â”€ df_test_ANON.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ gold_corpus
â”‚   â”‚   â””â”€â”€ labeled_spans_feature_engineered_ANON.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ pretraining_corpora *(not uploaded due to space constraints on GitHub, can be accessed by contacting the author or regenerated by running the preprocessing notebooks)*
â”‚   â”‚   â”œâ”€â”€ ALL_debates_with_turns_and_anon_18_april_debates.csv
â”‚   â”‚   â””â”€â”€ PLD_QA_debates_with_turns_and_anon_18_april_debates.csv
â”‚   â”‚
â”‚   â””â”€â”€ raw_annotations
â”‚       â”œâ”€â”€ df_all_ANON.csv
â”‚       â”œâ”€â”€ df_spans_ANON.csv
â”‚       â”œâ”€â”€ gold_df_ANON.csv
â”‚       â”œâ”€â”€ matches_df_ANON.csv
â”‚       â””â”€â”€ posthoc_containment_matches_ANON.csv
â”‚
â”œâ”€â”€ finetuning
â”‚   â”œâ”€â”€ models (model files and test results)
â”‚   â”‚   â”œâ”€â”€ dual
â”‚   â”‚   â””â”€â”€ mono
â”‚   â””â”€â”€ 1_finetune_classify_12_models.ipynb
â”‚
â”œâ”€â”€ preprocessing
â”‚   â”œâ”€â”€ preprocessing_post_annotation
â”‚   â”‚   â”œâ”€â”€ 1_IAA_and_get_labels.ipynb
â”‚   â”‚   â”œâ”€â”€ 2_get_metadata_on_labels.ipynb
â”‚   â”‚   â”œâ”€â”€ 3_feature_engineering.ipynb
â”‚   â”‚   â””â”€â”€ 4_inspections.ipynb
â”‚   â”‚
â”‚   â””â”€â”€ preprocessing_prior_annotation
â”‚       â”œâ”€â”€ temp_datasets *(not uploaded due to space constraints on GitHub, can be accessed by contacting the author or regenerated by running the preprocessing notebooks)*
â”‚       â”œâ”€â”€ 1_main_debate_cleaning.ipynb
â”‚       â”œâ”€â”€ 2_prep_data_for_streamlit.ipynb
â”‚       â””â”€â”€ 3_posthoc_cleaning.ipynb
â”‚
â”œâ”€â”€ pretraining *(not uploaded due to space constraints on GitHub, can be accessed by contacting the author or regenerated by running the modelling notebooks)*
â”‚   â”œâ”€â”€ models
â”‚   â”‚   â”œâ”€â”€ ALL
â”‚   â”‚   â””â”€â”€ PLD_QA
â”‚   â””â”€â”€ 1_pretraining_google_colab_script.ipynb
â”‚
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ” Detailed Description

### 1. `datasets/`

* **finetuning\_and\_test\_splits/**

  * CSV splits for training, validation, and testing the classification models (all anonymized: `_ANON.csv`).
* **gold\_corpus/**

  * The consolidated, feature-engineered gold-label dataset (all annotator IDs are anonymized).
* **pretraining\_corpora/**

  * Two large corpora of anonymized parliamentary debates used for continued pretraining of Danish BERT:

    * `ALL_debates_with_turns_and_anon_18_april_debates.csv`
    * `PLD_QA_debates_with_turns_and_anon_18_april_debates.csv`
* **raw\_annotations/**

  * Intermediate CSV files from the inter-annotator agreement (IAA) and spanâ€matching pipeline. All user/annotator IDs have been replaced with pseudonyms.

> **Note on Raw Parliamentary XMLs**
> The original Danish parliamentary XML files are **not** included in this repo (they take up too much space). These can be downloaded directly from [oda.ft.dk](https://oda.ft.dk/) and place them into a folder named `raw_parliamentary_xmls/` (or similar) at the root level. The preprocessing notebooks will automatically pick them up to generate temporary datasets used in later steps.

---

### 2. `preprocessing/`

* **preprocessing\_prior\_annotation/**

  1. `1_main_debate_cleaning.ipynb`

     * Cleans and parses raw XML debate files into a structured DataFrame.
  2. `2_prep_data_for_streamlit.ipynb`

     * Prepares data for a Streamlit annotation interface.
  3. `3_posthoc_cleaning.ipynb`

     * Further cleaning on XML-derived data after initial annotation.
* **preprocessing\_post\_annotation/**

  1. `1_IAA_and_get_labels.ipynb`

     * Inter-Annotator Agreement (IAA) pipeline: reading CSV annotations, computing Dice/containment matches, and generating a gold-standard span dataset.
  2. `2_get_metadata_on_labels.ipynb`

     * Joins metadata (e.g., speaker, party, date) to each labeled span.
  3. `3_feature_engineering.ipynb`

     * Extracts linguistic/rhetorical features (token counts, POS tags, role mentions, etc.) and generates a final â€œfeatureâ€engineeredâ€ gold DataFrame.
  4. `4_inspections.ipynb`

     * Exploratory analyses: annotation counts, spanâ€length distributions, label distributions, and lowâ€Dice/containment checks.

> **Anonymization**
> All notebooks that previously output true annotator IDs now replace them with anonymized IDs (8-character pseudonyms). Any CSVs containing user IDs have been overwritten with their `_ANON.csv` equivalents. This ensures participant privacy.

---

### 3. `pretraining/`

* **models/ALL & models/PLD\_QA/**

  * Checkpoints and tokenizer files for two separately continuedâ€pretrained BERT models:

    * `ALL`: Pretrained on the entire corpus of debates.
    * `PLD_QA`: Pretrained only on questionâ€answer subsets of the debates.
* `1_pretraining_google_colab_script.ipynb`

  * Google Colab script demonstrating how to continue pretraining Danishâ€BERT (Maltehb/danish-bert-botxo) on the `pretraining_corpora/`. Contains steps for data loading, tokenization, maskedâ€language modeling (MLM), trainingâ€argument setup, loss plotting, and saving the adapted model.

---

### 4. `finetuning/`

* **models/mono/**

  * Checkpoints & artifacts for monoâ€criterion fineâ€tuning experiments (only optimizing for Fâ‚ on â€œanswer vs. nonâ€answerâ€).
* **models/dual/**

  * Checkpoints & artifacts for dualâ€criterion fineâ€tuning experiments (monitoring both evalâ€loss and evalâ€Fâ‚ to trigger early stopping).
* `1_finetune_classify_12_models.ipynb`

  * Central notebook orchestrating:

    1. Loading the featureâ€engineered dataset (`df_coalesced_labels_feature_engineered_18_april_ANON.csv`)
    2. Stratified splits (outer train/test, inner train/val) based on `debate_unit_id`, speech length, and government/opposition status.
    3. Tokenization, dataset creation, and model setup (using either â€œALLâ€ or â€œPLD\_QAâ€ pretrained BERT, or baseline BERTâ€base).
    4. W\&B sweeps for hyperparameter optimization (monoâ€criterion & dualâ€criterion).
    5. â€œLoadâ€inâ€ evaluation of best sweep runs on the heldâ€out test set.
    6. â€œRetrainedâ€ experiments (reâ€train on combined train+val using best hyperparameters, then evaluate on test).
    7. Saving confusion matrices, classification reports, and raw predictions for each experiment.

---

## âš™ï¸ Getting Started

1. **Clone this repository**

   ```bash
   git clone https://github.com/<your_username>/rhetorical-strategy-detection.git
   cd rhetorical-strategy-detection
   ```

2. **Download Raw Parliamentary XML Files**

   * Visit [oda.ft.dk](https://oda.ft.dk/) to download the relevant Danish parliamentary debate XMLs.

3. **Anonymization Notice**

   * All annotator IDs in the notebooks and CSVs have been replaced with pseudonyms.

4. **Environment Setup**

   * Create a new Python 3.8+ environment (e.g., via `conda` or `venv`) and install dependencies as needed. Each notebook typically begins with a series of `!pip install ...` commands.
   * Recommended packages:

     ```
     pandas, numpy, scikit-learn, pytorch, transformers, datasets, evaluate,
     wandb (for hyperparameter sweeps), spacy (da_core_news_sm), matplotlib
     ```

5. **Running Notebooks**

   * **Preprocessing**:

     1. `preprocessing_prior_annotation/1_main_debate_cleaning.ipynb` â†’ parse XMLs into DataFrames.
     2. `preprocessing_prior_annotation/2_prep_data_for_streamlit.ipynb` â†’ set up annotation interface data.
     3. `preprocessing_prior_annotation/3_posthoc_cleaning.ipynb` â†’ clean after annotation.
   * **Annotation Pipeline**:

     1. `preprocessing_post_annotation/1_IAA_and_get_labels.ipynb` â†’ compute IAA and generate gold spans.
     2. `preprocessing_post_annotation/2_get_metadata_on_labels.ipynb` â†’ enrich spans with metadata (turnâ€level, speaker, party, etc.).
     3. `preprocessing_post_annotation/3_feature_engineering.ipynb` â†’ compute linguistic/rhetorical features and TFâ€IDF.
     4. `preprocessing_post_annotation/4_inspections.ipynb` â†’ exploratory data analysis and sanity checks.
   * **Pretraining**:

     * `pretraining/1_pretraining_google_colab_script.ipynb` â†’ run MLM pretraining on either `ALL` or `PLD_QA` corpora.
   * **Fine-Tuning**:

     * `finetuning/1_finetune_classify_12_models.ipynb` â†’ carry out monoâ€ and dualâ€criterion fineâ€tuning, logging to W\&B, and evaluating on test data.

6. **Results & Models**

   * Pretrained & fineâ€tuned model checkpoints are stored under:

     ```
     pretraining/models/ALL/
     pretraining/models/PLD_QA/
     finetuning/models/mono/
     finetuning/models/dual/
     ```
   * Final metrics, classification reports, and confusion matrices for each experiment are saved within the `finetuning/models/â€¦` folders as well as under `results_test_*` directories.

---

## ğŸ”’ Anonymization & Privacy

* **Annotator User IDs**: Whenever a notebook originally output real user IDs, those IDs have been removed or replaced with 8-character pseudonyms.

---

## ğŸš€ Tips & Tricks

* **GPU Usage**:

  * Most pretraining and fineâ€tuning notebooks assume a GPU access (e.g., in Google Colab). The first cell checks for `nvidia-smi` and prints GPU info.

* **W\&B Sweeps**:

  * Both monoâ€criterion and dualâ€criterion hyperparameter searches use Weights & Biases.

---

## ğŸ™ Acknowledgments

This repository and the underlying research were conducted as part of my masterâ€™s in Cognitive Science at \[Aarhus University]. Special thanks to all student annotators and advisors who contributed to the goldâ€span annotation process.
---

*Happy coding & happy analyzing!* ğŸ‰

