# 🏛️ Rhetorical-Strategy-Detection
 
## 📚 Project Overview

This repository contains all code, notebooks, and data pipelines developed for my master’s thesis in Cognitive Science, titled **DISARM: Deception Identification and Statement Assessment in Rhetorical Messaging Using NLP**. The goal of this project is to build an end-to-end NLP pipeline for detecting rhetorical strategies—particularly distinguishing “Answers” from “Non-Answers”—in Danish parliamentary question-answer exchanges (QAEs).

---

## 📁 Folder Structure

Below is a high-level overview of the directory structure:

```
rhetorical-strategy-detection
│
├── datasets
│   ├── finetuning_and_test_splits
│   │   ├── df_train_ANON.csv
│   │   ├── df_val_ANON.csv
│   │   └── df_test_ANON.csv
│   │
│   ├── gold_corpus
│   │   └── labeled_spans_feature_engineered_ANON.csv
│   │
│   ├── pretraining_corpora *(not uploaded due to space constraints on GitHub, can be accessed by contacting the author or regenerated by running the preprocessing notebooks)*
│   │   ├── ALL_debates_with_turns_and_anon_18_april_debates.csv
│   │   └── PLD_QA_debates_with_turns_and_anon_18_april_debates.csv
│   │
│   └── raw_annotations
│       ├── df_all_ANON.csv
│       ├── df_spans_ANON.csv
│       ├── gold_df_ANON.csv
│       ├── matches_df_ANON.csv
│       └── posthoc_containment_matches_ANON.csv
│
├── finetuning
│   ├── models (model files and test results)
│   │   ├── dual
│   │   └── mono
│   └── 1_finetune_classify_12_models.ipynb
│
├── preprocessing
│   ├── preprocessing_post_annotation
│   │   ├── 1_IAA_and_get_labels.ipynb
│   │   ├── 2_get_metadata_on_labels.ipynb
│   │   ├── 3_feature_engineering.ipynb
│   │   └── 4_inspections.ipynb
│   │
│   └── preprocessing_prior_annotation
│       ├── temp_datasets *(not uploaded due to space constraints on GitHub, can be accessed by contacting the author or regenerated by running the preprocessing notebooks)*
│       ├── 1_main_debate_cleaning.ipynb
│       ├── 2_prep_data_for_streamlit.ipynb
│       └── 3_posthoc_cleaning.ipynb
│
├── pretraining *(not uploaded due to space constraints on GitHub, can be accessed by contacting the author or regenerated by running the modelling notebooks)*
│   ├── models
│   │   ├── ALL
│   │   └── PLD_QA
│   └── 1_pretraining_google_colab_script.ipynb
│
├── .gitattributes
├── .gitignore
└── README.md
```

---

## 🔍 Detailed Description

### 1. `datasets/`

* **finetuning\_and\_test\_splits/**

  * CSV splits for training, validation, and testing the classification models (all anonymized: `_ANON.csv`).
* **gold\_corpus/**

  * The consolidated, feature-engineered gold-label dataset (all annotator IDs are anonymized).
* **pretraining\_corpora/**

  * Two large corpora of anonymized parliamentary debates used for continued pretraining of Danish BERT:

    * `ALL_debates_with_turns_and_anon_18_april_debates.csv`
    * `PLD_QA_debates_with_turns_and_anon_18_april_debates.csv`
* **raw\_annotations/**

  * Intermediate CSV files from the inter-annotator agreement (IAA) and span‐matching pipeline. All user/annotator IDs have been replaced with pseudonyms.

> **Note on Raw Parliamentary XMLs**
> The original Danish parliamentary XML files are **not** included in this repo (they take up too much space). These can be downloaded directly from [oda.ft.dk](https://oda.ft.dk/) and place them into a folder named `raw_parliamentary_xmls/` (or similar) at the root level. The preprocessing notebooks will automatically pick them up to generate temporary datasets used in later steps.

---

### 2. `preprocessing/`

* **preprocessing\_prior\_annotation/**

  1. `1_main_debate_cleaning.ipynb`

     * Cleans and parses raw XML debate files into a structured DataFrame.
  2. `2_prep_data_for_streamlit.ipynb`

     * Prepares data for a Streamlit annotation interface.
  3. `3_posthoc_cleaning.ipynb`

     * Further cleaning on XML-derived data after initial annotation.
* **preprocessing\_post\_annotation/**

  1. `1_IAA_and_get_labels.ipynb`

     * Inter-Annotator Agreement (IAA) pipeline: reading CSV annotations, computing Dice/containment matches, and generating a gold-standard span dataset.
  2. `2_get_metadata_on_labels.ipynb`

     * Joins metadata (e.g., speaker, party, date) to each labeled span.
  3. `3_feature_engineering.ipynb`

     * Extracts linguistic/rhetorical features (token counts, POS tags, role mentions, etc.) and generates a final “feature‐engineered” gold DataFrame.
  4. `4_inspections.ipynb`

     * Exploratory analyses: annotation counts, span‐length distributions, label distributions, and low‐Dice/containment checks.

> **Anonymization**
> All notebooks that previously output true annotator IDs now replace them with anonymized IDs (8-character pseudonyms). Any CSVs containing user IDs have been overwritten with their `_ANON.csv` equivalents. This ensures participant privacy.

---

### 3. `pretraining/`

* **models/ALL & models/PLD\_QA/**

  * Checkpoints and tokenizer files for two separately continued‐pretrained BERT models:

    * `ALL`: Pretrained on the entire corpus of debates.
    * `PLD_QA`: Pretrained only on question‐answer subsets of the debates.
* `1_pretraining_google_colab_script.ipynb`

  * Google Colab script demonstrating how to continue pretraining Danish‐BERT (Maltehb/danish-bert-botxo) on the `pretraining_corpora/`. Contains steps for data loading, tokenization, masked‐language modeling (MLM), training‐argument setup, loss plotting, and saving the adapted model.

---

### 4. `finetuning/`

* **models/mono/**

  * Checkpoints & artifacts for mono‐criterion fine‐tuning experiments (only optimizing for F₁ on “answer vs. non‐answer”).
* **models/dual/**

  * Checkpoints & artifacts for dual‐criterion fine‐tuning experiments (monitoring both eval‐loss and eval‐F₁ to trigger early stopping).
* `1_finetune_classify_12_models.ipynb`

  * Central notebook orchestrating:

    1. Loading the feature‐engineered dataset (`df_coalesced_labels_feature_engineered_18_april_ANON.csv`)
    2. Stratified splits (outer train/test, inner train/val) based on `debate_unit_id`, speech length, and government/opposition status.
    3. Tokenization, dataset creation, and model setup (using either “ALL” or “PLD\_QA” pretrained BERT, or baseline BERT‐base).
    4. W\&B sweeps for hyperparameter optimization (mono‐criterion & dual‐criterion).
    5. “Load‐in” evaluation of best sweep runs on the held‐out test set.
    6. “Retrained” experiments (re‐train on combined train+val using best hyperparameters, then evaluate on test).
    7. Saving confusion matrices, classification reports, and raw predictions for each experiment.

---

## ⚙️ Getting Started

1. **Clone this repository**

   ```bash
   git clone https://github.com/<your_username>/rhetorical-strategy-detection.git
   cd rhetorical-strategy-detection
   ```

2. **Download Raw Parliamentary XML Files**

   * Visit [oda.ft.dk](https://oda.ft.dk/) to download the relevant Danish parliamentary debate XMLs.

3. **Anonymization Notice**

   * All annotator IDs in the notebooks and CSVs have been replaced with pseudonyms.

4. **Environment Setup**

   * Create a new Python 3.8+ environment (e.g., via `conda` or `venv`) and install dependencies as needed. Each notebook typically begins with a series of `!pip install ...` commands.
   * Recommended packages:

     ```
     pandas, numpy, scikit-learn, pytorch, transformers, datasets, evaluate,
     wandb (for hyperparameter sweeps), spacy (da_core_news_sm), matplotlib
     ```

5. **Running Notebooks**

   * **Preprocessing**:

     1. `preprocessing_prior_annotation/1_main_debate_cleaning.ipynb` → parse XMLs into DataFrames.
     2. `preprocessing_prior_annotation/2_prep_data_for_streamlit.ipynb` → set up annotation interface data.
     3. `preprocessing_prior_annotation/3_posthoc_cleaning.ipynb` → clean after annotation.
   * **Annotation Pipeline**:

     1. `preprocessing_post_annotation/1_IAA_and_get_labels.ipynb` → compute IAA and generate gold spans.
     2. `preprocessing_post_annotation/2_get_metadata_on_labels.ipynb` → enrich spans with metadata (turn‐level, speaker, party, etc.).
     3. `preprocessing_post_annotation/3_feature_engineering.ipynb` → compute linguistic/rhetorical features and TF‐IDF.
     4. `preprocessing_post_annotation/4_inspections.ipynb` → exploratory data analysis and sanity checks.
   * **Pretraining**:

     * `pretraining/1_pretraining_google_colab_script.ipynb` → run MLM pretraining on either `ALL` or `PLD_QA` corpora.
   * **Fine-Tuning**:

     * `finetuning/1_finetune_classify_12_models.ipynb` → carry out mono‐ and dual‐criterion fine‐tuning, logging to W\&B, and evaluating on test data.

6. **Results & Models**

   * Pretrained & fine‐tuned model checkpoints are stored under:

     ```
     pretraining/models/ALL/
     pretraining/models/PLD_QA/
     finetuning/models/mono/
     finetuning/models/dual/
     ```
   * Final metrics, classification reports, and confusion matrices for each experiment are saved within the `finetuning/models/…` folders as well as under `results_test_*` directories.

---

## 🔒 Anonymization & Privacy

* **Annotator User IDs**: Whenever a notebook originally output real user IDs, those IDs have been removed or replaced with 8-character pseudonyms.

---

## 🚀 Tips & Tricks

* **GPU Usage**:

  * Most pretraining and fine‐tuning notebooks assume a GPU access (e.g., in Google Colab). The first cell checks for `nvidia-smi` and prints GPU info.

* **W\&B Sweeps**:

  * Both mono‐criterion and dual‐criterion hyperparameter searches use Weights & Biases.

---

## 🙏 Acknowledgments

This repository and the underlying research were conducted as part of my master’s in Cognitive Science at \[Aarhus University]. Special thanks to all student annotators and advisors who contributed to the gold‐span annotation process.
---

*Happy coding & happy analyzing!* 🎉

