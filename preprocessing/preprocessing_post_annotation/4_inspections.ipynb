{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ca21f5",
   "metadata": {},
   "source": [
    "# Gold-spans inspections for tables in-paper üîç \n",
    "\n",
    "This notebook loads the gold span dataset and computes descriptive statistics on annotation behavior, span/text lengths, and label distributions. It covers:\n",
    "1. Annotation counts per user and study group, and normalized annotation rates.\n",
    "2. Text statistics (word, character, sentence counts) for each annotated span and its full debate context.\n",
    "3. Debate-unit‚Äìlevel metrics (annotations per unit, unit lengths).\n",
    "4. Label frequency and normalization by number of debate units, both overall and stratified by user/study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5fb5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load spans\n",
    "df_spans = pd.read_csv(\"output/prelim_dataframes/df_spans.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0dcf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- Text processing functions ---\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_characters(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_sentences(text):\n",
    "    return sum(1 for char in text if char in ['.', '!', '?'])\n",
    "\n",
    "# --- Annotation counts ---\n",
    "annotations_per_user = df_spans.groupby('user_id').size()\n",
    "print(f\"Average number of annotations per user: {annotations_per_user.mean():.2f}\")\n",
    "print(f\"Standard deviation of annotations per user: {annotations_per_user.std():.2f}\")\n",
    "print(\"\\nNumber of annotations per user:\")\n",
    "print(annotations_per_user)\n",
    "\n",
    "if 'user_id_study' in df_spans.columns:\n",
    "    annotations_per_study = df_spans.groupby('user_id_study').size()\n",
    "    print(\"\\nNumber of annotations per study type:\")\n",
    "    print(annotations_per_study)\n",
    "    print(f\"\\nAverage per study: {annotations_per_study.mean():.2f}\")\n",
    "    print(f\"Standard deviation per study: {annotations_per_study.std():.2f}\")\n",
    "\n",
    "    # Annotations per user per study\n",
    "    annotations_per_study_user = df_spans.groupby(['user_id_study', 'user_id']).size().groupby('user_id_study').agg(['mean', 'std'])\n",
    "    print(\"\\nAverage annotations per user per study type (with SD):\")\n",
    "    print(annotations_per_study_user)\n",
    "\n",
    "    # breakdown by predefined study types\n",
    "    for study in ['linguistics', 'cognitive science', 'rhetorics']:\n",
    "        study_data = df_spans[df_spans['user_id_study'] == study]\n",
    "        user_counts = study_data.groupby('user_id').size()\n",
    "        print(f\"\\n{study.capitalize()} - Avg: {user_counts.mean():.2f}, SD: {user_counts.std():.2f}\")\n",
    "\n",
    "# --- Span text stats ---\n",
    "for col, source in [('span', 'annotation'), ('full_text', 'full_text')]:\n",
    "    if col == 'full_text':\n",
    "        df_spans['cleaned_full_text'] = df_spans[col].apply(lambda x: re.sub(r'^\\[\\d+\\]\\s', '', x))\n",
    "        base_col = 'cleaned_full_text'\n",
    "    else:\n",
    "        base_col = col\n",
    "\n",
    "    df_spans[f'word_count_{source}'] = df_spans[base_col].apply(count_words)\n",
    "    df_spans[f'character_count_{source}'] = df_spans[base_col].apply(count_characters)\n",
    "    df_spans[f'sentence_count_{source}'] = df_spans[base_col].apply(count_sentences)\n",
    "\n",
    "    print(f\"\\n{source.capitalize()} statistics:\")\n",
    "    for metric in ['word', 'character', 'sentence']:\n",
    "        col_name = f'{metric}_count_{source}'\n",
    "        print(f\"{metric.capitalize()} count - Mean: {df_spans[col_name].mean():.2f}, SD: {df_spans[col_name].std():.2f}\")\n",
    "\n",
    "# --- Debate unit analysis ---\n",
    "annotations_per_debate_unit = df_spans.groupby('debate_unit_id').size()\n",
    "print(f\"\\nEach debate unit received an average of {annotations_per_debate_unit.mean():.2f} annotations (SD = {annotations_per_debate_unit.std():.2f})\")\n",
    "\n",
    "total_unique_units = df_spans['debate_unit_id'].nunique()\n",
    "print(f\"Total unique debate units: {total_unique_units}\")\n",
    "\n",
    "debate_units_per_user = df_spans.groupby('user_id')['debate_unit_id'].nunique()\n",
    "print(f\"Average debate units per annotator: {debate_units_per_user.mean():.2f}, SD: {debate_units_per_user.std():.2f}\")\n",
    "\n",
    "# --- Debate unit text length stats ---\n",
    "for metric_func, name in [(count_words, 'word'), (count_characters, 'character'), (count_sentences, 'sentence')]:\n",
    "    df_spans[f'{name}_count_unit'] = df_spans['full_text'].apply(metric_func)\n",
    "    grouped = df_spans.groupby('debate_unit_id')[f'{name}_count_unit'].mean()\n",
    "    print(f\"Avg {name} count per debate unit: {grouped.mean():.2f} (SD = {grouped.std():.2f})\")\n",
    "\n",
    "# --- Normalized annotations ---\n",
    "normalized_annotations = annotations_per_user / debate_units_per_user\n",
    "print(f\"\\nAverage normalized annotations per user: {normalized_annotations.mean():.2f}\")\n",
    "print(f\"Standard deviation: {normalized_annotations.std():.2f}\")\n",
    "\n",
    "if 'user_id_study' in df_spans.columns:\n",
    "    norm_mean = df_spans.groupby('user_id_study').apply(\n",
    "        lambda x: x.groupby('user_id')['debate_unit_id'].nunique().mean()\n",
    "    )\n",
    "    norm_std = df_spans.groupby('user_id_study').apply(\n",
    "        lambda x: x.groupby('user_id')['debate_unit_id'].nunique().std()\n",
    "    )\n",
    "    print(\"\\nAverage normalized annotations per study type:\")\n",
    "    print(norm_mean)\n",
    "    print(\"\\nStandard deviation of normalized annotations per study type:\")\n",
    "    print(norm_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15393c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each debate unit received an average of 9.06 annotations with an SD of 5.61.\n",
      "A total of 294 unique debate units were annotated.\n",
      "On average, each annotator contributed to 34.68 unique debate units, with a SD of 20.93.\n",
      "In terms of length, the debate units had an average of M = 617.12 words (SD = 319.53),\n",
      "M = 3479.46 characters (SD = 1837.19), and\n",
      "M = 29.50 sentences (SD = 16.40).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Calculate the number of annotations per debate unit\n",
    "annotations_per_debate_unit = df_spans.groupby('debate_unit_id').size()\n",
    "\n",
    "# Step 2: Calculate the average and standard deviation for annotations per debate unit (TODO: this here there is code somewhere else to fix, i think in IAA_and_get_labels)\n",
    "avg_annotations_per_debate_unit = annotations_per_debate_unit.mean()\n",
    "std_annotations_per_debate_unit = annotations_per_debate_unit.std()\n",
    "\n",
    "# Step 3: Calculate the total number of unique debate units annotated\n",
    "total_unique_debate_units = df_spans['debate_unit_id'].nunique()\n",
    "\n",
    "# Step 4: Calculate the number of unique debate units contributed by each annotator\n",
    "debate_units_per_annotator = df_spans.groupby('user_id')['debate_unit_id'].nunique()\n",
    "\n",
    "# Step 5: Calculate the average and standard deviation for the number of unique debate units contributed by each annotator\n",
    "avg_debate_units_per_annotator = debate_units_per_annotator.mean()\n",
    "std_debate_units_per_annotator = debate_units_per_annotator.std()\n",
    "\n",
    "# Step 6: Calculate word count, character count, and sentence count for each debate unit\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_characters(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_sentences(text):\n",
    "    sentence_endings = ['.', '!', '?']\n",
    "    return sum(1 for char in text if char in sentence_endings)\n",
    "\n",
    "# Apply the functions to the 'full_text' column of each debate unit\n",
    "df_spans['word_count'] = df_spans['full_text'].apply(count_words)\n",
    "df_spans['character_count'] = df_spans['full_text'].apply(count_characters)\n",
    "df_spans['sentence_count'] = df_spans['full_text'].apply(count_sentences)\n",
    "\n",
    "# Step 7: Calculate averages for word count, character count, and sentence count\n",
    "avg_word_count_per_debate_unit = df_spans.groupby('debate_unit_id')['word_count'].mean().mean()\n",
    "avg_character_count_per_debate_unit = df_spans.groupby('debate_unit_id')['character_count'].mean().mean()\n",
    "avg_sentence_count_per_debate_unit = df_spans.groupby('debate_unit_id')['sentence_count'].mean().mean()\n",
    "\n",
    "# Step 8: Calculate standard deviations for word count, character count, and sentence count\n",
    "std_word_count_per_debate_unit = df_spans.groupby('debate_unit_id')['word_count'].mean().std()\n",
    "std_character_count_per_debate_unit = df_spans.groupby('debate_unit_id')['character_count'].mean().std()\n",
    "std_sentence_count_per_debate_unit = df_spans.groupby('debate_unit_id')['sentence_count'].mean().std()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Each debate unit received an average of {avg_annotations_per_debate_unit:.2f} annotations with an SD of {std_annotations_per_debate_unit:.2f}.\")\n",
    "print(f\"A total of {total_unique_debate_units} unique debate units were annotated.\")\n",
    "print(f\"On average, each annotator contributed to {avg_debate_units_per_annotator:.2f} unique debate units, with a SD of {std_debate_units_per_annotator:.2f}.\")\n",
    "print(f\"In terms of length, the debate units had an average of M = {avg_word_count_per_debate_unit:.2f} words (SD = {std_word_count_per_debate_unit:.2f}),\")\n",
    "print(f\"M = {avg_character_count_per_debate_unit:.2f} characters (SD = {std_character_count_per_debate_unit:.2f}), and\")\n",
    "print(f\"M = {avg_sentence_count_per_debate_unit:.2f} sentences (SD = {std_sentence_count_per_debate_unit:.2f}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a3c7e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw counts of labels across all data:\n",
      "label\n",
      "answer            1099\n",
      "evasion            810\n",
      "attack             428\n",
      "self_promotion     242\n",
      "stretch             86\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages of labels across all data:\n",
      "label\n",
      "answer            41.238274\n",
      "evasion           30.393996\n",
      "attack            16.060038\n",
      "self_promotion     9.080675\n",
      "stretch            3.227017\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Raw counts of labels per user:\n",
      "user_id   label         \n",
      "2WJLAP22  answer             33\n",
      "          evasion            19\n",
      "          self_promotion     13\n",
      "          stretch             6\n",
      "          attack              4\n",
      "                           ... \n",
      "Z1NERR2S  answer            177\n",
      "          attack             98\n",
      "          evasion            76\n",
      "          self_promotion     24\n",
      "          stretch             5\n",
      "Name: count, Length: 93, dtype: int64\n",
      "\n",
      "Average number of labels per user:\n",
      "user_id\n",
      "2WJLAP22    15.00\n",
      "3P6E0LDX    27.20\n",
      "4AXSP923    10.60\n",
      "7MXM4GWL    13.80\n",
      "8ER9GXMV    68.60\n",
      "8UN6GI5O    18.25\n",
      "8WPTBUXI    39.75\n",
      "EJ567288    21.60\n",
      "F83UNDMC    21.60\n",
      "HMTGKXEX    40.20\n",
      "KANC4O8Z    19.20\n",
      "KT0HEM1R    11.80\n",
      "S1Y5M4BF    11.60\n",
      "SJD5FK02    34.80\n",
      "SLV0JUYJ    36.80\n",
      "VBANNHFR    22.60\n",
      "VFQZVTHB    14.80\n",
      "VI2DI9MW    40.40\n",
      "Z1NERR2S    76.00\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Raw counts of labels per study type:\n",
      "user_id_study      label         \n",
      "cognitive science  answer            225\n",
      "                   evasion            79\n",
      "                   attack             65\n",
      "                   self_promotion     46\n",
      "                   stretch            13\n",
      "linguistics        answer            653\n",
      "                   evasion           580\n",
      "                   attack            300\n",
      "                   self_promotion    157\n",
      "                   stretch            54\n",
      "rhetorics          answer            221\n",
      "                   evasion           151\n",
      "                   attack             63\n",
      "                   self_promotion     39\n",
      "                   stretch            19\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average number of labels per study type:\n",
      "user_id_study\n",
      "cognitive science     85.6\n",
      "linguistics          348.8\n",
      "rhetorics             98.6\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Normalized label counts per user (per debate unit):\n",
      "user_id   label         \n",
      "2WJLAP22  answer            2.357143\n",
      "          evasion           1.357143\n",
      "          self_promotion    0.928571\n",
      "          stretch           0.428571\n",
      "          attack            0.285714\n",
      "                              ...   \n",
      "Z1NERR2S  answer            2.132530\n",
      "          attack            1.180723\n",
      "          evasion           0.915663\n",
      "          self_promotion    0.289157\n",
      "          stretch           0.060241\n",
      "Length: 93, dtype: float64\n",
      "\n",
      "Normalized label counts per study type (per debate unit):\n",
      "user_id_study      label         \n",
      "cognitive science  answer            1.923077\n",
      "                   evasion           0.675214\n",
      "                   attack            0.555556\n",
      "                   self_promotion    0.393162\n",
      "                   stretch           0.111111\n",
      "linguistics        answer            2.492366\n",
      "                   evasion           2.213740\n",
      "                   attack            1.145038\n",
      "                   self_promotion    0.599237\n",
      "                   stretch           0.206107\n",
      "rhetorics          answer            2.084906\n",
      "                   evasion           1.424528\n",
      "                   attack            0.594340\n",
      "                   self_promotion    0.367925\n",
      "                   stretch           0.179245\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Now check distributions of labels in the raw spans\n",
    "import pandas as pd\n",
    "\n",
    "# Labels we're interested in\n",
    "labels = ['answer', 'stretch', 'evasion', 'self_promotion', 'attack']\n",
    "\n",
    "# Step 1: Count raw occurrences of each label across all data\n",
    "label_counts = df_spans['label'].value_counts()\n",
    "\n",
    "# Step 2: Calculate percentages for each label\n",
    "label_percentages = df_spans['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Step 3: Count raw occurrences per user\n",
    "label_counts_per_user = df_spans.groupby('user_id')['label'].value_counts()\n",
    "\n",
    "# Step 4: Calculate the average number of each label per user\n",
    "avg_labels_per_user = label_counts_per_user.groupby('user_id').mean()\n",
    "\n",
    "# Step 5: Count raw occurrences per study type\n",
    "if 'user_id_study' in df_spans.columns:\n",
    "    label_counts_per_study = df_spans.groupby('user_id_study')['label'].value_counts()\n",
    "\n",
    "    # Step 6: Calculate the average number of each label per study type\n",
    "    avg_labels_per_study = label_counts_per_study.groupby('user_id_study').mean()\n",
    "\n",
    "# Step 7: Normalize label counts by the number of unique debate units annotated by each user\n",
    "# Count unique debate units per user\n",
    "debate_units_per_user = df_spans.groupby('user_id')['debate_unit_id'].nunique()\n",
    "\n",
    "# Normalize the counts by dividing by the number of unique debate units per user\n",
    "normalized_label_counts_per_user = label_counts_per_user / debate_units_per_user\n",
    "\n",
    "# Step 8: Normalize label counts by the number of unique debate units per study type\n",
    "# Count unique debate units per study type\n",
    "debate_units_per_study = df_spans.groupby('user_id_study')['debate_unit_id'].nunique()\n",
    "\n",
    "# Normalize label counts by the number of unique debate units per study type\n",
    "normalized_label_counts_per_study = label_counts_per_study / debate_units_per_study\n",
    "\n",
    "# Step 9: Print the results\n",
    "\n",
    "# Raw counts of labels\n",
    "print(\"Raw counts of labels across all data:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Percentages of labels\n",
    "print(\"\\nPercentages of labels across all data:\")\n",
    "print(label_percentages)\n",
    "\n",
    "# Raw counts of labels per user\n",
    "print(\"\\nRaw counts of labels per user:\")\n",
    "print(label_counts_per_user)\n",
    "\n",
    "# Average number of labels per user\n",
    "print(\"\\nAverage number of labels per user:\")\n",
    "print(avg_labels_per_user)\n",
    "\n",
    "# Raw counts of labels per study type\n",
    "if 'user_id_study' in df_spans.columns:\n",
    "    print(\"\\nRaw counts of labels per study type:\")\n",
    "    print(label_counts_per_study)\n",
    "\n",
    "    # Average number of labels per study type\n",
    "    print(\"\\nAverage number of labels per study type:\")\n",
    "    print(avg_labels_per_study)\n",
    "\n",
    "# Normalized counts of labels per user (per debate unit)\n",
    "print(\"\\nNormalized label counts per user (per debate unit):\")\n",
    "print(normalized_label_counts_per_user)\n",
    "\n",
    "# Normalized counts of labels per study type (per debate unit)\n",
    "if 'user_id_study' in df_spans.columns:\n",
    "    print(\"\\nNormalized label counts per study type (per debate unit):\")\n",
    "    print(normalized_label_counts_per_study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "167b25f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spans_w_counts = df_spans.copy()\n",
    "df_spans_w_counts.to_csv(\"output/prelim_dataframes/inspections/df_spans_w_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking \n",
    "import pandas as pd\n",
    "\n",
    "checking = pd.read_csv(\"/Users/pbrams/Desktop/AARHUS_UNIVERSITY/kandidat/thesis_work/annotations/output/for_pretraining/ALL_debates_with_turns_and_anon_18_april_debates_wo_labelledrows.csv\")\n",
    "checking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
