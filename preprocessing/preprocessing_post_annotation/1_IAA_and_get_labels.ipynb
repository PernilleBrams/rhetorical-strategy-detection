{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-annotator agreement span-matching and consolidation pipeline for span-based annotations 🧪\n",
    "This notebook evaluates inter-annotator agreement (IAA) on span-based labels in Danish political debates and builds a consolidated “gold” dataset of agreed spans.  \n",
    "It loads each annotator’s CSV, computes pairwise soft‐match metrics (Dice ≥ 0.6 + containment), extracts longest common substrings, and outputs a final gold‐span dataset for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GOAL: Evaluate inter-annotator agreement (IAA) for span-based annotations and create a reliable gold dataset of agreed-upon labels.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PIPELINE OVERVIEW:\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "1. Load CSV annotation files for each annotator.\n",
    "   - Each CSV includes: debate_unit_id, full_text, and multiple label columns (e.g., answer, evasion).\n",
    "   - Spans for each label are semicolon-separated text segments.\n",
    "\n",
    "2. Normalize & combine data:\n",
    "   - Extract each labeled span per annotator and explode into one row per (debate_unit_id, label, span, annotator).\n",
    "   - Combine all annotators' data into one master dataframe.\n",
    "\n",
    "3. Compute pairwise soft span matching:\n",
    "   - For each (debate_unit_id, label), compare spans between all annotators who annotated that debate.\n",
    "   - Compute Dice coefficient (soft match) between spans.\n",
    "   - Also check for containment: whether one span is fully contained in the other.\n",
    "\n",
    "4. Agreement Criteria:\n",
    "   - Dice >= 0.6 → considered a soft match.\n",
    "   - Containment = True → means high textual agreement.\n",
    "   - Label must be the same.\n",
    "\n",
    "5. Generate gold span set:\n",
    "   - If at least two annotators agree (Dice >= 0.6 AND same label AND containment = True), extract shared span.\n",
    "   - Save as a new row in a gold dataset.\n",
    "\n",
    "6. Export results:\n",
    "   - Full dataframe with matched spans, Dice, containment, label, etc.\n",
    "   - Clean gold dataset with agreed spans & labels for model training.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# REASONS FOR CHOICES:\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "✅ Dice threshold of 0.6:\n",
    "   - Literature (e.g. Zhang et al. 2017) uses soft matching thresholds around 0.5-0.7.\n",
    "   - 0.6 balances precision and recall — it allows for minor differences in highlight boundaries.\n",
    "\n",
    "✅ Containment check:\n",
    "   - Especially useful for annotators who consistently over/under-highlight.\n",
    "   - If one span is entirely inside another, it's a strong agreement signal.\n",
    "\n",
    "✅ Use of shared span:\n",
    "   - Ensures high precision and conservative gold data.\n",
    "   - Simplifies pipeline and avoids overfitting to annotator habits.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "### ---------- UTILITIES ---------- ###\n",
    "\n",
    "def dice_coefficient(a, b):\n",
    "    a_tokens, b_tokens = set(a.split()), set(b.split())\n",
    "    if not a_tokens or not b_tokens:\n",
    "        return 0.0\n",
    "    overlap = len(a_tokens & b_tokens)\n",
    "    return 2 * overlap / (len(a_tokens) + len(b_tokens))\n",
    "\n",
    "def is_contained(span1, span2):\n",
    "    return span1 in span2 or span2 in span1\n",
    "\n",
    "def get_overlap_substring(a, b):\n",
    "    a, b = a.strip(), b.strip()\n",
    "    if a in b:\n",
    "        return a\n",
    "    elif b in a:\n",
    "        return b\n",
    "    else:\n",
    "        match = SequenceMatcher(None, a, b).find_longest_match(0, len(a), 0, len(b))\n",
    "        return a[match.a: match.a + match.size].strip()\n",
    "\n",
    "### ---------- STEP 1: Load & Normalize ---------- ###\n",
    "\n",
    "def extract_spans(df, label_col):\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Only split if the value is not missing\n",
    "        spans = str(row[label_col]).split(';') if pd.notna(row[label_col]) else []\n",
    "        for span in spans:\n",
    "            cleaned = span.strip()\n",
    "            # Skip if cleaned is empty or equals \"nan\" (case insensitive)\n",
    "            if cleaned and cleaned.lower() != \"nan\":\n",
    "                rows.append({\n",
    "                    'user_id': row['user_id'],\n",
    "                    'user_id_study': row['user_id_study'],\n",
    "                    'debate_unit_id': row['debate_unit_id'],\n",
    "                    'label': label_col,\n",
    "                    'span': cleaned,\n",
    "                    'full_text': row['full_text']\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def load_all_annotations(data_folder):\n",
    "    all_csvs = glob.glob(os.path.join(data_folder, '*.csv'))\n",
    "    all_dfs = []\n",
    "    for path in all_csvs:\n",
    "        df = pd.read_csv(path)\n",
    "        # Treat _SH_PLD etc. as the same user\n",
    "        user_id = os.path.basename(path).split('Annotations - ')[-1].split('.')[0].split('_')[0]\n",
    "        df['user_id'] = user_id\n",
    "        all_dfs.append(df)\n",
    "    return pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "def build_master_span_table(df, labels):\n",
    "    return pd.concat([extract_spans(df, label) for label in labels], ignore_index=True)\n",
    "\n",
    "### ---------- STEP 2: Compute Soft Matches ---------- ###\n",
    "\n",
    "def compute_soft_matches(df_spans, dice_threshold=0.6):\n",
    "    gold_rows = []\n",
    "    match_rows = []\n",
    "\n",
    "    grouped = df_spans.groupby(['debate_unit_id', 'label'])\n",
    "    for (debate_id, label), group in grouped:\n",
    "        annos = list(group.to_dict('records'))\n",
    "        for a1, a2 in itertools.combinations(annos, 2):\n",
    "            if a1['user_id'] == a2['user_id']:\n",
    "                continue\n",
    "            dice = dice_coefficient(a1['span'], a2['span'])\n",
    "            containment = is_contained(a1['span'], a2['span'])\n",
    "            match_rows.append({\n",
    "                'debate_unit_id': debate_id,\n",
    "                'label': label,\n",
    "                'annotator_1': a1['user_id'],\n",
    "                'annotator_2': a2['user_id'],\n",
    "                'span_1': a1['span'],\n",
    "                'span_2': a2['span'],\n",
    "                'dice': dice,\n",
    "                'contained': containment\n",
    "            })\n",
    "            if dice >= dice_threshold and containment:\n",
    "                shared_span = get_overlap_substring(a1['span'], a2['span'])\n",
    "                if shared_span.strip():\n",
    "                    gold_rows.append({\n",
    "                        'debate_unit_id': debate_id,\n",
    "                        'label': label,\n",
    "                        'span': shared_span,\n",
    "                        'source_annotators': f\"{a1['user_id']},{a2['user_id']}\"\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(match_rows), pd.DataFrame(gold_rows).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧾 Load all annotations\n",
    "data_folder = './data' \n",
    "df_all = load_all_annotations(data_folder)\n",
    "\n",
    "# Remove linebreaks\n",
    "import re\n",
    "\n",
    "def clean_linebreaks(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Replace colon followed by any whitespace (including newline) with \": \"\n",
    "    text = re.sub(r':\\s+', ': ', text)\n",
    "    return text.strip()\n",
    "\n",
    "cols_to_fix = ['full_text', 'answer', 'stretch', 'evasion', 'self_promotion', 'attack']\n",
    "\n",
    "for col in cols_to_fix:\n",
    "    if col in df_all.columns:\n",
    "        df_all[col] = df_all[col].apply(clean_linebreaks)\n",
    "\n",
    "\n",
    "# Replace ; in full text with , \n",
    "df_all['full_text'] = df_all['full_text'].str.replace(';', ',', regex=False) # For Streamlit data reasons\n",
    "\n",
    "# Replace p\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove linebreaks\n",
    "import re\n",
    "\n",
    "def clean_linebreaks(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Replace colon followed by any whitespace (including newline) with \": \"\n",
    "    text = re.sub(r':\\s+', ': ', text)\n",
    "    return text.strip()\n",
    "\n",
    "cols_to_fix = ['full_text', 'answer', 'stretch', 'evasion', 'self_promotion', 'attack']\n",
    "\n",
    "for col in cols_to_fix:\n",
    "    if col in df_all.columns:\n",
    "        df_all[col] = df_all[col].apply(clean_linebreaks)\n",
    "\n",
    "\n",
    "# Replace ; in full text with , \n",
    "df_all['full_text'] = df_all['full_text'].str.replace(';', ',', regex=False) # For Streamlit data reasons\n",
    "\n",
    "# Replace party stuff again for safety\n",
    "import re\n",
    "\n",
    "# Define a mapping of party names (including historical names) to pseudonyms\n",
    "party_pseudonyms = {\n",
    "    # Socialdemokratiet\n",
    "    \"Socialdemokratiet\": \"Parti_A\",\n",
    "    \"Socialdemokraterne\": \"Parti_A\",\n",
    "    \"Socialdemokraternes\": \"Parti_As\",\n",
    "    \"Socialdemokratiets\": \"Parti_As\",\n",
    "    \"Socialdemokratisk\": \"Parti_As\",\n",
    "    \"Socialdemokrater\": \"Parti_A\",\n",
    "\n",
    "    # Venstre\n",
    "    \"Venstre\": \"Parti_B\",\n",
    "    \"Venstres\": \"Parti_Bs\",\n",
    "\n",
    "    # Radikale Venstre\n",
    "    \"Radikale Venstre\": \"Parti_C\",\n",
    "    \"Det Radikale Venstre\": \"Parti_C\",\n",
    "    #\"Radikale\": \"Parti_C\",\n",
    "    \"Radikales\": \"Parti_Cs\",\n",
    "    \"De Radikale\": \"Parti_C\",\n",
    "    \"De Radikales\": \"Parti_Cs\",\n",
    "    \"Radikale\": \"Parti_C\",\n",
    "\n",
    "    # Konservative Folkeparti\n",
    "    \"Konservative Folkeparti\": \"Parti_D\",\n",
    "    \"Det Konservative Folkeparti\": \"Parti_D\",\n",
    "    \"Konservative\": \"Parti_D\",\n",
    "    \"Konservatives\": \"Parti_Ds\",\n",
    "    \"De Konservative\": \"Parti_D\",\n",
    "    \"De Konservatives\": \"Parti_Ds\",\n",
    "    \"konservativ side\": \"Parti_Ds side\",\n",
    "\n",
    "    # Socialistisk Folkeparti\n",
    "    \"Socialistisk Folkeparti\": \"Parti_E\",\n",
    "    \"Socialistisk Folkepartis\": \"Parti_Es\",\n",
    "    \"Socialistiske Folkeparti\": \"Parti_E\",\n",
    "    \"Socialistiskes\": \"Parti_Es\",\n",
    "    \"SF\": \"Parti_E\",\n",
    "    \"SFs\": \"Parti_Es\",\n",
    "    \"SF's\": \"Parti_Es\",\n",
    "\n",
    "    # Dansk Folkeparti\n",
    "    \"Dansk Folkeparti\": \"Parti_F\",\n",
    "    \"Dansk Folkepartis\": \"Parti_Fs\",\n",
    "\n",
    "    # Fremskridtspartiet (Historisk DF-navn)\n",
    "    \"Fremskridtspartiet\": \"Parti_F\",\n",
    "    \"Fremskridtspartiets\": \"Parti_Fs\",\n",
    "\n",
    "    # Enhedslisten\n",
    "    \"Enhedslisten\": \"Parti_G\",\n",
    "    \"Enhedslistens\": \"Parti_Gs\",\n",
    "    \"Rød-Grøn Alliance\": \"Parti_G\",\n",
    "    \"Rød-Grønne Alliance\": \"Parti_G\",\n",
    "\n",
    "    # Liberal Alliance\n",
    "    \"Liberal Alliance\": \"Parti_H\",\n",
    "    \"Liberale Alliance\": \"Parti_H\",\n",
    "    \"Liberal Alliances\": \"Parti_Hs\",\n",
    "    \"Liberales\": \"Parti_Hs\",  # Genitive form\n",
    "\n",
    "    # Ny Alliance (Historisk før LA)\n",
    "    \"Ny Alliance\": \"Parti_H\",\n",
    "    \"Ny Alliances\": \"Parti_Hs\",\n",
    "\n",
    "    # Alternativet\n",
    "    \"Alternativet\": \"Parti_I\",\n",
    "    \"Alternativets\": \"Parti_Is\",\n",
    "\n",
    "    # Danmarksdemokraterne\n",
    "    \"Danmarksdemokraterne\": \"Parti_J\",\n",
    "    \"Danmarksdemokraternes\": \"Parti_Js\",\n",
    "\n",
    "    # Nye Borgerlige\n",
    "    \"Nye Borgerlige\": \"Parti_K\",\n",
    "    \"Nye Borgerliges\": \"Parti_Ks\",\n",
    "\n",
    "    # Frie Grønne\n",
    "    \"Frie Grønne\": \"Parti_L\",\n",
    "    \"De Frie Grønne\": \"Parti_L\",\n",
    "    \"Frie Grønnes\": \"Parti_Ls\",\n",
    "\n",
    "    # Kristendemokraterne\n",
    "    \"Kristendemokraterne\": \"Parti_M\",\n",
    "    \"Kristendemokraternes\": \"Parti_Ms\",\n",
    "    \"De Kristne Demokrater\": \"Parti_M\",\n",
    "    \"Kristendemokratiet\": \"Parti_M\",\n",
    "    \"Kristendemokratiets\": \"Parti_Ms\",\n",
    "}\n",
    "\n",
    "# Compile regex pattern to match any of the party names (case insensitive)\n",
    "party_pattern = re.compile(r'\\b(' + '|'.join(re.escape(party) for party in party_pseudonyms.keys()) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Function to replace party names with pseudonyms\n",
    "def replace_party_names(text):\n",
    "    if pd.isna(text):  # Handle missing values\n",
    "        return text\n",
    "    \n",
    "    # Perform case-insensitive replacement while preserving original case\n",
    "    return party_pattern.sub(lambda match: party_pseudonyms.get(match.group(0), \n",
    "                                                                party_pseudonyms.get(match.group(0).title(), \n",
    "                                                                match.group(0))), text)\n",
    "\n",
    "# Apply function to the utterance and text cols\n",
    "df_all[\"full_text\"] = df_all[\"full_text\"].astype(str).apply(replace_party_names)\n",
    "df_all[\"answer\"] = df_all[\"answer\"].astype(str).apply(replace_party_names)\n",
    "df_all[\"stretch\"] = df_all[\"stretch\"].astype(str).apply(replace_party_names)\n",
    "df_all[\"evasion\"] = df_all[\"evasion\"].astype(str).apply(replace_party_names)\n",
    "df_all[\"self_promotion\"] = df_all[\"self_promotion\"].astype(str).apply(replace_party_names)\n",
    "df_all[\"attack\"] = df_all[\"attack\"].astype(str).apply(replace_party_names)\n",
    "df_all[\"other\"] = df_all[\"other\"].astype(str).apply(replace_party_names)\n",
    "\n",
    "print(\"✅ Party names replaced with pseudonyms, including historical names.\")\n",
    "\n",
    "df_all.to_csv(\"output/df_all.csv\")\n",
    "\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get demographics on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mapping for user_id to study field\n",
    "# This originally held a full list which is removed here since these were the original user-ids and not the re-pseudonymized (so that annotators' cannot see contributions from other annotators, if they happened to see or hear their usernames during the study)\n",
    "\n",
    "user_id_to_study = {\n",
    " ### anonymized ###\n",
    "}\n",
    "\n",
    "# Create the new column 'user_id_study' based on the user_id\n",
    "df_all['user_id_study'] = df_all['user_id'].map(user_id_to_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run cleaner functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 Supported labels\n",
    "labels = ['answer', 'evasion', 'attack', 'self_promotion', 'stretch']\n",
    "\n",
    "# 🔄 Explode into one span per row\n",
    "df_spans = build_master_span_table(df_all, labels)\n",
    "\n",
    "# 🔍 Compute Dice + containment matching\n",
    "matches_df, gold_df = compute_soft_matches(df_spans, dice_threshold=0.6)\n",
    "\n",
    "# Save them\n",
    "df_spans.to_csv(\"output/prelim_dataframes/df_spans.csv\")\n",
    "matches_df.to_csv(\"output/prelim_dataframes/matches_df.csv\")\n",
    "gold_df.to_csv(\"output/prelim_dataframes/gold_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look\n",
    "gold_df # before we have 232, then 733, then 736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also take a look at this one\n",
    "df_spans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_study</th>\n",
       "      <th>annotators</th>\n",
       "      <th>total_annotations</th>\n",
       "      <th>mean_per_annotator</th>\n",
       "      <th>sd_per_annotator</th>\n",
       "      <th>mean_normalized</th>\n",
       "      <th>sd_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cognitive science</td>\n",
       "      <td>5</td>\n",
       "      <td>428.0</td>\n",
       "      <td>85.60</td>\n",
       "      <td>50.05</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>10</td>\n",
       "      <td>1744.0</td>\n",
       "      <td>174.40</td>\n",
       "      <td>106.37</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rhetorics</td>\n",
       "      <td>4</td>\n",
       "      <td>493.0</td>\n",
       "      <td>123.25</td>\n",
       "      <td>68.09</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Overall</td>\n",
       "      <td>19</td>\n",
       "      <td>2665.0</td>\n",
       "      <td>140.26</td>\n",
       "      <td>92.35</td>\n",
       "      <td>4.29</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id_study  annotators  total_annotations  mean_per_annotator  \\\n",
       "0  cognitive science           5              428.0               85.60   \n",
       "1        linguistics          10             1744.0              174.40   \n",
       "2          rhetorics           4              493.0              123.25   \n",
       "3            Overall          19             2665.0              140.26   \n",
       "\n",
       "   sd_per_annotator  mean_normalized  sd_normalized  \n",
       "0             50.05             3.76           1.24  \n",
       "1            106.37             4.18           0.99  \n",
       "2             68.09             5.23           2.22  \n",
       "3             92.35             4.29           1.39  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the Annotator Contributions (continued in next chunk)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spans dataframe\n",
    "df_spans_check = pd.read_csv(\"output/prelim_dataframes/df_spans.csv\")\n",
    "\n",
    "# Compute per-annotator stats\n",
    "user_stats = (\n",
    "    df_spans_check\n",
    "    .groupby(['user_id_study', 'user_id'])\n",
    "    .agg(\n",
    "        total_annotations=('span', 'count'),\n",
    "        unique_units=('debate_unit_id', pd.Series.nunique)\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "user_stats['normalized_per_unit'] = user_stats['total_annotations'] / user_stats['unique_units']\n",
    "\n",
    "# Aggregate per study\n",
    "study_stats = (\n",
    "    user_stats\n",
    "    .groupby('user_id_study')\n",
    "    .agg(\n",
    "        annotators=('user_id', 'nunique'),\n",
    "        total_annotations=('total_annotations', 'sum'),\n",
    "        mean_per_annotator=('total_annotations', 'mean'),\n",
    "        sd_per_annotator=('total_annotations', 'std'),\n",
    "        mean_normalized=('normalized_per_unit', 'mean'),\n",
    "        sd_normalized=('normalized_per_unit', 'std')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Overall row\n",
    "overall = pd.Series({\n",
    "    'user_id_study': 'Overall',\n",
    "    'annotators': user_stats['user_id'].nunique(),\n",
    "    'total_annotations': user_stats['total_annotations'].sum(),\n",
    "    'mean_per_annotator': user_stats['total_annotations'].mean(),\n",
    "    'sd_per_annotator': user_stats['total_annotations'].std(),\n",
    "    'mean_normalized': user_stats['normalized_per_unit'].mean(),\n",
    "    'sd_normalized': user_stats['normalized_per_unit'].std()\n",
    "})\n",
    "final_stats = pd.concat([study_stats, overall.to_frame().T], ignore_index=True)\n",
    "\n",
    "# Round numeric columns to two decimals (except annotators)\n",
    "for col in ['total_annotations', 'mean_per_annotator', 'sd_per_annotator', 'mean_normalized', 'sd_normalized']:\n",
    "    final_stats[col] = final_stats[col].astype(float).round(2)\n",
    "\n",
    "# Ensure annotators is integer\n",
    "final_stats['annotators'] = final_stats['annotators'].astype(int)\n",
    "\n",
    "# Display\n",
    "final_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting relative rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting relative rates\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Count spans per (user, unit)\n",
    "unit_user = (\n",
    "    df_spans_check\n",
    "    .groupby(['debate_unit_id','user_id'])\n",
    "    .size()\n",
    "    .rename('user_spans')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 2) Compute average spans per unit across all users\n",
    "unit_density = (\n",
    "    unit_user\n",
    "    .groupby('debate_unit_id')['user_spans']\n",
    "    .mean()\n",
    "    .rename('mean_spans_per_unit')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 3) Merge back so each (user, unit) row has actual vs. expected\n",
    "merged = unit_user.merge(unit_density, on='debate_unit_id')\n",
    "\n",
    "# 4) Summarize per‐user\n",
    "user_density = (\n",
    "    merged\n",
    "    .groupby('user_id')\n",
    "    .agg(\n",
    "        total_spans=('user_spans','sum'),\n",
    "        expected_spans=('mean_spans_per_unit','sum'),\n",
    "        units_seen=('debate_unit_id','nunique')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 5) Compute relative rate\n",
    "user_density['relative_rate'] = user_density['total_spans'] / user_density['expected_spans']\n",
    "\n",
    "# 6) Bring in each user’s study field\n",
    "user_info = (\n",
    "    df_spans_check[['user_id','user_id_study']]\n",
    "    .drop_duplicates()\n",
    ")\n",
    "user_density = user_density.merge(user_info, on='user_id')\n",
    "\n",
    "# 7) Aggregate by study\n",
    "study_density = (\n",
    "    user_density\n",
    "    .groupby('user_id_study')\n",
    "    .agg(\n",
    "        annotators=('user_id','nunique'),\n",
    "        mean_relative_rate=('relative_rate','mean'),\n",
    "        sd_relative_rate=('relative_rate','std')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 8) z‐score the per-user rates across all annotators\n",
    "user_density['relative_rate_z'] = (\n",
    "    (user_density['relative_rate'] - user_density['relative_rate'].mean())\n",
    "    / user_density['relative_rate'].std()\n",
    ")\n",
    "\n",
    "# 9) Round for presentation\n",
    "study_density[['mean_relative_rate','sd_relative_rate']] = study_density[['mean_relative_rate','sd_relative_rate']].round(2)\n",
    "user_density['relative_rate'] = user_density['relative_rate'].round(2)\n",
    "user_density['relative_rate_z'] = user_density['relative_rate_z'].round(2)\n",
    "\n",
    "print(study_density)\n",
    "print(user_density[['user_id','user_id_study','relative_rate','relative_rate_z']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates (before I had 150, then 556, then 557\n",
    "gold_df_no_dupes = gold_df.drop_duplicates(subset = ['span'])\n",
    "gold_df_no_dupes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Total unique labeled spans: 557\n",
      "📊 Label Distribution (% of unique labeled spans):\n",
      "       Percentage  proportion\n",
      "0          answer       50.09\n",
      "1         evasion       32.32\n",
      "2          attack       11.31\n",
      "3  self_promotion        6.10\n",
      "4         stretch        0.18\n",
      "📊 Label Count:\n",
      "            Count  count\n",
      "0          answer    279\n",
      "1         evasion    180\n",
      "2          attack     63\n",
      "3  self_promotion     34\n",
      "4         stretch      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/y6c4hxp96_j_rq3k86806lgr0000gn/T/ipykernel_4518/2089956132.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gold_df_no_dupes['span'] = gold_df_no_dupes['span'].str.strip()\n",
      "/var/folders/rp/y6c4hxp96_j_rq3k86806lgr0000gn/T/ipykernel_4518/2089956132.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gold_df_no_dupes['label'] = gold_df_no_dupes['label'].str.strip()\n"
     ]
    }
   ],
   "source": [
    "# Ensure no trailing spaces etc.\n",
    "gold_df_no_dupes['span'] = gold_df_no_dupes['span'].str.strip()\n",
    "gold_df_no_dupes['label'] = gold_df_no_dupes['label'].str.strip()\n",
    "\n",
    "# Total unique training examples\n",
    "num_unique = len(gold_df_no_dupes)\n",
    "print(f\"🔢 Total unique labeled spans: {num_unique}\")\n",
    "\n",
    "# Label distribution (as % of unique spans)\n",
    "label_dist = (\n",
    "    gold_df_no_dupes['label']\n",
    "    .value_counts(normalize=True)\n",
    "    .mul(100.0)  # ensures float\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'Label', 'label': 'Percentage'})\n",
    ")\n",
    "\n",
    "print(\"📊 Label Distribution (% of unique labeled spans):\")\n",
    "print(label_dist)\n",
    "\n",
    "label_counts = (\n",
    "    gold_df_no_dupes['label']\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'Label', 'label': 'Count'})\n",
    ")\n",
    "\n",
    "print(\"📊 Label Count:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tiebreak for cases where span is the same but the label is not the same (formerly these were 157 rows, now theyre 455)\n",
    "def get_disagreement_cases(df_spans, dice_threshold=0.6):\n",
    "    rows = []\n",
    "    grouped = df_spans.groupby('debate_unit_id')\n",
    "    \n",
    "    for debate_id, group in grouped:\n",
    "        annos = list(group.to_dict('records'))\n",
    "        for a1, a2 in itertools.combinations(annos, 2):\n",
    "            if a1['user_id'] == a2['user_id']:\n",
    "                continue\n",
    "            dice = dice_coefficient(a1['span'], a2['span'])\n",
    "            contained = is_contained(a1['span'], a2['span'])\n",
    "            if dice >= dice_threshold and contained and a1['label'] != a2['label']:\n",
    "                rows.append({\n",
    "                    'debate_unit_id': debate_id,\n",
    "                    'annotator_1': a1['user_id'],\n",
    "                    'label_1': a1['label'],\n",
    "                    'span_1': a1['span'],\n",
    "                    'annotator_2': a2['user_id'],\n",
    "                    'label_2': a2['label'],\n",
    "                    'span_2': a2['span'],\n",
    "                    'dice': dice,\n",
    "                    'contained': contained\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "tie_break_df = get_disagreement_cases(df_spans, dice_threshold=0.6)\n",
    "tie_break_df # disse skal ses igennem af en tredje annotør (gør imorgen?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Outputs saved:\n",
      "- span_matches_with_dice.csv\n",
      "- gold_df_no_dupes.csv\n",
      "- tie_break_cases.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 💾 Save outputs\n",
    "matches_df.to_csv('/Users/pbrams/Desktop/AARHUS_UNIVERSITY/kandidat/thesis_work/annotations/output/span_matches_with_dice.csv', index=False)\n",
    "gold_df_no_dupes.to_csv('/Users/pbrams/Desktop/AARHUS_UNIVERSITY/kandidat/thesis_work/annotations/output/gold_df_no_dupes.csv', index=False)\n",
    "tie_break_df.to_csv(\"/Users/pbrams/Desktop/AARHUS_UNIVERSITY/kandidat/thesis_work/annotations/output/tie_break_cases.csv\", index=False)\n",
    "\n",
    "print(\"✅ Done. Outputs saved:\")\n",
    "print(\"- span_matches_with_dice.csv\")\n",
    "print(\"- gold_df_no_dupes.csv\")\n",
    "print(\"- tie_break_cases.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra processing for DebateUnits who were seen by 4 annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have some cases where there has been 4 annotators on a debate unit, meaning that annotator A and B have marked a span as answer, and annotator C and D have marked a span as an answer, but the two spans are partially overlapping.\n",
    "# 1) keep longest span of the two - assumes that more context = better signal, and keeps the full rehtorical move if some annotators included it. Option 2 is use the shared (overlapping) span. I extract the longest common subspan shared between annotators (like the current method, stays consistent)\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "\n",
    "def extract_clauses(text):\n",
    "    \"\"\"Split text into small clause units based on sentence delimiters.\"\"\"\n",
    "    return re.split(r'[.,;]', text)\n",
    "\n",
    "def contains_clause_overlap(s1, s2):\n",
    "    \"\"\"Check if any clause in s1 exists in s2 or vice versa.\"\"\"\n",
    "    clauses1 = [cl.strip() for cl in extract_clauses(s1) if cl.strip()]\n",
    "    clauses2 = [cl.strip() for cl in extract_clauses(s2) if cl.strip()]\n",
    "    return any(clause in s2 for clause in clauses1) or any(clause in s1 for clause in clauses2)\n",
    "\n",
    "def get_longest_common_substring(a, b):\n",
    "    \"\"\"Get the longest character-based common substring between two spans.\"\"\"\n",
    "    match = SequenceMatcher(None, a, b).find_longest_match(0, len(a), 0, len(b))\n",
    "    return a[match.a: match.a + match.size].strip()\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.strip().split())\n",
    "\n",
    "\n",
    "def collapse_spans_by_clause_overlap(gold_df, min_words=3):\n",
    "    final_rows = []\n",
    "    grouped = gold_df.groupby(['debate_unit_id', 'label'])\n",
    "\n",
    "    for (debate_id, label), group in grouped:\n",
    "        spans = group['span'].tolist()\n",
    "        annotators = group['source_annotators'].tolist()\n",
    "        used_pairs = set()\n",
    "        span_to_annotators = defaultdict(set)\n",
    "        span_to_notes = defaultdict(list)\n",
    "\n",
    "        # Store all candidate shared spans\n",
    "        candidate_spans = []\n",
    "\n",
    "        for i in range(len(spans)):\n",
    "            for j in range(i + 1, len(spans)):\n",
    "                s1, s2 = spans[i], spans[j]\n",
    "                uid1 = annotators[i]\n",
    "                uid2 = annotators[j]\n",
    "                pair_id = tuple(sorted([uid1, uid2]))\n",
    "\n",
    "                if pair_id in used_pairs:\n",
    "                    continue\n",
    "\n",
    "                if contains_clause_overlap(s1, s2):\n",
    "                    shared = get_longest_common_substring(s1, s2)\n",
    "                    if shared.strip() and word_count(shared) >= min_words:\n",
    "                        candidate_spans.append({\n",
    "                            'shared_span': shared.strip(),\n",
    "                            'annotators': set(uid1.split(',')) | set(uid2.split(',')),\n",
    "                            'note': f\"Clause overlap from {uid1} & {uid2}\"\n",
    "                        })\n",
    "                        used_pairs.add(pair_id)\n",
    "\n",
    "        if candidate_spans:\n",
    "            # Sort candidates by span length (descending), keep only longest one\n",
    "            longest = max(candidate_spans, key=lambda x: word_count(x['shared_span']))\n",
    "            final_rows.append({\n",
    "                'debate_unit_id': debate_id,\n",
    "                'label': label,\n",
    "                'span': longest['shared_span'],\n",
    "                'source_annotators': \",\".join(sorted(longest['annotators'])),\n",
    "                'construction_note': longest['note']\n",
    "            })\n",
    "        else:\n",
    "            # No matches — keep individual spans\n",
    "            for span, annotator in zip(spans, annotators):\n",
    "                final_rows.append({\n",
    "                    'debate_unit_id': debate_id,\n",
    "                    'label': label,\n",
    "                    'span': span.strip(),\n",
    "                    'source_annotators': annotator,\n",
    "                    'construction_note': \"Singleton span — no overlapping pairs\" # Unique or unmatched span\"\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(final_rows)\n",
    "\n",
    "collapsed_gold_df = collapse_spans_by_clause_overlap(gold_df, min_words=4) # dupe df?\n",
    "collapsed_gold_df.to_csv('/Users/pbrams/Desktop/AARHUS_UNIVERSITY/kandidat/thesis_work/annotations/output/collapsed_gold_df.csv', index=False)\n",
    "collapsed_gold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce duplicate answers with the same span for pairs of annotators: \n",
    "collapsed_gold_df['span'] = collapsed_gold_df['span'].str.strip()\n",
    "collapsed_gold_df['label'] = collapsed_gold_df['label'].str.strip()\n",
    "collapsed_gold_df['source_annotators'] = collapsed_gold_df['source_annotators'].str.strip()\n",
    "\n",
    "# Group by span identity\n",
    "coalesced = (\n",
    "    collapsed_gold_df\n",
    "    .groupby(['debate_unit_id', 'label', 'span'], as_index=False)\n",
    "    .agg({\n",
    "        'source_annotators': lambda x: ','.join(sorted(set(','.join(x).split(',')))),\n",
    "        'construction_note': lambda x: '; '.join(sorted(set(x)))\n",
    "    })\n",
    ")\n",
    "\n",
    "# Remove accidental double spaces, clean up\n",
    "coalesced['source_annotators'] = coalesced['source_annotators'].str.replace(r'\\s+', '', regex=True)\n",
    "coalesced['construction_note'] = coalesced['construction_note'].str.strip()\n",
    "\n",
    "# Label distribution (as % of unique spans)\n",
    "label_dist = (\n",
    "    coalesced['label']\n",
    "    .value_counts(normalize=True)\n",
    "    .mul(100.0)  # ensures float\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'Label', 'label': 'Percentage'})\n",
    ")\n",
    "\n",
    "print(\"📊 Label Distribution (% of unique labeled spans):\")\n",
    "print(label_dist)\n",
    "\n",
    "label_counts = (\n",
    "    coalesced['label']\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'Label', 'label': 'Count'})\n",
    ")\n",
    "\n",
    "print(\"📊 Label Count:\")\n",
    "print(label_counts)\n",
    "\n",
    "coalesced.to_csv('/Users/pbrams/Desktop/AARHUS_UNIVERSITY/kandidat/thesis_work/annotations/output/coalesced_gold_df.csv', index=False)\n",
    "coalesced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add tiebreaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work a bit on tie-break\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Helper functions (redefine for safety)\n",
    "def extract_clauses(text):\n",
    "    \"\"\"Split text into small clause units based on sentence delimiters.\"\"\"\n",
    "    return re.split(r'[.,;]', text)\n",
    "\n",
    "def contains_clause_overlap(s1, s2):\n",
    "    \"\"\"Check if any clause in s1 exists in s2 or vice versa.\"\"\"\n",
    "    clauses1 = [cl.strip() for cl in extract_clauses(s1) if cl.strip()]\n",
    "    clauses2 = [cl.strip() for cl in extract_clauses(s2) if cl.strip()]\n",
    "    return any(clause in s2 for clause in clauses1) or any(clause in s1 for clause in clauses2)\n",
    "\n",
    "def get_longest_common_substring(a, b):\n",
    "    \"\"\"Get the longest character-based common substring between two spans.\"\"\"\n",
    "    match = SequenceMatcher(None, a, b).find_longest_match(0, len(a), 0, len(b))\n",
    "    return a[match.a: match.a + match.size].strip()\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.strip().split())\n",
    "\n",
    "# Process the tiebreak DataFrame\n",
    "def process_tiebreak_df(df_tiebreak, min_words=4):\n",
    "    rows = []\n",
    "    for _, row in df_tiebreak.iterrows():\n",
    "        s1 = row['span_1']\n",
    "        s2 = row['span_2']\n",
    "        \n",
    "        # Calculate the shared span if there is clause overlap\n",
    "        if contains_clause_overlap(s1, s2):\n",
    "            shared = get_longest_common_substring(s1, s2)\n",
    "        else:\n",
    "            shared = \"\"\n",
    "        \n",
    "        # Check if the shared span meets the minimum word threshold\n",
    "        if shared and word_count(shared) >= min_words:\n",
    "            shared_span = shared.strip()\n",
    "            note = f\"Clause overlap from {row['annotator_1']} & {row['annotator_2']}\"\n",
    "        else:\n",
    "            # If not, we fallback to a combined span \n",
    "            shared_span = s1.strip() + \" / \" + s2.strip()\n",
    "            note = f\"No sufficient clause overlap from {row['annotator_1']} & {row['annotator_2']}\"\n",
    "        \n",
    "        # Create a combined label (e.g., \"evasion + attack\")\n",
    "        disagreement_label = row['label_1'].strip() + \" + \" + row['label_2'].strip()\n",
    "        \n",
    "        # Combine annotators (if there are multiple, split and sort)\n",
    "        annos = sorted(set(row['annotator_1'].split(\",\") + row['annotator_2'].split(\",\")))\n",
    "        source_ann = \",\".join(annos)\n",
    "        \n",
    "        rows.append({\n",
    "            'debate_unit_id': row['debate_unit_id'],\n",
    "            'label': disagreement_label,\n",
    "            'span': shared_span,\n",
    "            'source_annotators': source_ann,\n",
    "            'construction_note': note,\n",
    "            'disagreement': True\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Get tiebreak df\n",
    "tie_break_df = pd.read_csv('/Users/pbrams/Desktop/AARHUS_UNIVERSITY/kandidat/thesis_work/annotations/output/tie_break_cases.csv')\n",
    "\n",
    "# List of valid disagreement labels (if one of these is paired with 'answer', we skip it)\n",
    "valid_labels = ['evasion', 'attack', 'self_promotion', 'stretch']\n",
    "\n",
    "# Step 2: Filter tie_break_df for rows where both label_1 and label_2 are in valid_labels.\n",
    "tie_break_valid = tie_break_df[\n",
    "    tie_break_df['label_1'].isin(valid_labels) & tie_break_df['label_2'].isin(valid_labels)\n",
    "].copy()\n",
    "\n",
    "tie_break_valid\n",
    "\n",
    "# Now, process tie_break_df\n",
    "tiebreak_processed_df = process_tiebreak_df(tie_break_valid, min_words=4)\n",
    "\n",
    "# Filter out the no sufficient overlap\n",
    "tiebreak_processed_df = tiebreak_processed_df[\n",
    "    ~tiebreak_processed_df['construction_note'].str.contains(\"No sufficient clause overlap\", case=False, na=False)\n",
    "]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "tiebreak_processed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>count</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer</td>\n",
       "      <td>evasion</td>\n",
       "      <td>252</td>\n",
       "      <td>56.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>evasion</td>\n",
       "      <td>attack</td>\n",
       "      <td>50</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>evasion</td>\n",
       "      <td>self_promotion</td>\n",
       "      <td>42</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>answer</td>\n",
       "      <td>self_promotion</td>\n",
       "      <td>32</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>answer</td>\n",
       "      <td>attack</td>\n",
       "      <td>23</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>evasion</td>\n",
       "      <td>stretch</td>\n",
       "      <td>21</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>answer</td>\n",
       "      <td>stretch</td>\n",
       "      <td>13</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>attack</td>\n",
       "      <td>self_promotion</td>\n",
       "      <td>8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>attack</td>\n",
       "      <td>stretch</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>self_promotion</td>\n",
       "      <td>stretch</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label_1         label_2  count  percent\n",
       "1          answer         evasion    252     56.2\n",
       "6         evasion          attack     50     11.2\n",
       "7         evasion  self_promotion     42      9.4\n",
       "2          answer  self_promotion     32      7.1\n",
       "0          answer          attack     23      5.1\n",
       "8         evasion         stretch     21      4.7\n",
       "3          answer         stretch     13      2.9\n",
       "4          attack  self_promotion      8      1.8\n",
       "5          attack         stretch      4      0.9\n",
       "9  self_promotion         stretch      3      0.7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a tie break cases overview\n",
    "import pandas as pd\n",
    "\n",
    "tie_break_df = pd.read_csv('/Users/pbrams/Desktop/AARHUS_UNIVERSITY/kandidat/thesis_work/annotations/output/tie_break_cases.csv')\n",
    "tie_break_df\n",
    "\n",
    "# Create a cross-tabulation of disagreements\n",
    "confusion = (\n",
    "    tie_break_df\n",
    "    .groupby(['label_1', 'label_2'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values('count', ascending=False)\n",
    ")\n",
    "\n",
    "# Add a percentage column\n",
    "total_disagreements = confusion['count'].sum()\n",
    "confusion['percent'] = (confusion['count'] / total_disagreements * 100).round(1)\n",
    "\n",
    "# Sort by descending count (or percent)\n",
    "confusion = confusion.sort_values('count', ascending=False)\n",
    "\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to append this to coalesced now, but also priotizie the ones in coalesced if there are any ones in there that the tiebreak also overlaps with\n",
    "\n",
    "# Making a version of coalesced_gold_df\n",
    "import pandas as pd\n",
    "coalesced = pd.read_csv('/Users/pbrams/Desktop/AARHUS_UNIVERSITY/kandidat/thesis_work/annotations/output/coalesced_gold_df.csv')\n",
    "\n",
    "# Step 1: Ensure gold_df has a disagreement_status column, and mark gold agreements as False.\n",
    "coalesced['disagreement'] = False\n",
    "\n",
    "# Ensure that coalesced (gold df) has the expected columns and flag\n",
    "coalesced['span'] = coalesced['span'].str.strip()\n",
    "coalesced['label'] = coalesced['label'].str.strip()\n",
    "coalesced['source_annotators'] = coalesced['source_annotators'].str.strip()\n",
    "\n",
    "# Define a minimum word threshold for considering a subpart as \"existing\"\n",
    "min_words_threshold = 4\n",
    "\n",
    "# Prepare a list to accumulate tiebreak rows that should be added\n",
    "rows_to_add = []\n",
    "\n",
    "# Loop through each row in the tiebreak DataFrame\n",
    "for idx, tb_row in tiebreak_processed_df.iterrows():\n",
    "    debate_id = tb_row['debate_unit_id']\n",
    "    tb_span = tb_row['span'].strip()\n",
    "    \n",
    "    # Flag to determine whether to skip this tiebreak row\n",
    "    skip = False\n",
    "    \n",
    "    # Check if this debate_unit_id exists in the gold (coalesced) DataFrame\n",
    "    if debate_id in coalesced['debate_unit_id'].values:\n",
    "        # Get the subset of gold rows for the current debate unit\n",
    "        gold_subset = coalesced[coalesced['debate_unit_id'] == debate_id]\n",
    "        \n",
    "        # Check each gold row in this subset\n",
    "        for _, gold_row in gold_subset.iterrows():\n",
    "            gold_span = gold_row['span'].strip()\n",
    "            \n",
    "            # 3) If the span exactly exists, then skip this tiebreak row\n",
    "            if gold_span == tb_span:\n",
    "                skip = True\n",
    "                break\n",
    "            \n",
    "            # 4) Check if subparts of the tiebreak span already exist:\n",
    "            # Compute the longest common substring between tb_span and gold_span\n",
    "            common = get_longest_common_substring(tb_span, gold_span)\n",
    "            if common and word_count(common) >= min_words_threshold:\n",
    "                skip = True\n",
    "                break\n",
    "    # If debate_unit_id doesn't exist in gold, or if none of the checks triggered a skip,\n",
    "    # we add the row.\n",
    "    if not skip:\n",
    "        # Ensure the disagreement flag is set to True (even if it might already be True)\n",
    "        tb_row['disagreement'] = True\n",
    "        rows_to_add.append(tb_row)\n",
    "\n",
    "# Convert the collected rows to a DataFrame\n",
    "if rows_to_add:\n",
    "    appended_tiebreak_df = pd.DataFrame(rows_to_add)\n",
    "else:\n",
    "    appended_tiebreak_df = pd.DataFrame(columns=tiebreak_processed_df.columns)\n",
    "\n",
    "# display the rows to be added\n",
    "print(\"Rows from tiebreak that will be appended:\")\n",
    "appended_tiebreak_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Append tie_break_valid rows to gold_df\n",
    "gold_df_augmented = pd.concat([coalesced, appended_tiebreak_df], ignore_index=True)\n",
    "gold_df_augmented.to_csv('/Users/pbrams/Desktop/AARHUS_UNIVERSITY/kandidat/thesis_work/annotations/output/coalesced_gold_augmented_df.csv', index=False)\n",
    "\n",
    "gold_df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nI feel like\\nContainment is a stronger structural signal than token overlap: if one annotator's span is fully included in another's, \\nthey are often referring to the same rhetorical move — just with broader or narrower emphasis.\\n\\nDice can be sensitive to short spans and stopword mismatch, meaning it may unfairly penalize what is otherwise a clear agreement.\\n\\n\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the ones where dice is low, but containment is fulfilled for manual review.\n",
    "'''\n",
    "I feel like\n",
    "Containment is a stronger structural signal than token overlap: if one annotator's span is fully included in another's, \n",
    "they are often referring to the same rhetorical move — just with broader or narrower emphasis.\n",
    "\n",
    "Dice can be sensitive to short spans and stopword mismatch, meaning it may unfairly penalize what is otherwise a clear agreement.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_containment_only_matches_with_label_merge(matches_df, gold_df, min_words=3):\n",
    "    matches_df['span_1'] = matches_df['span_1'].str.strip()\n",
    "    matches_df['span_2'] = matches_df['span_2'].str.strip()\n",
    "    gold_spans = set(\n",
    "        (row['debate_unit_id'], row['label'], row['span'].strip())\n",
    "        for _, row in gold_df.iterrows()\n",
    "    )\n",
    "\n",
    "    output_rows = []\n",
    "\n",
    "    for _, row in matches_df.iterrows():\n",
    "        if not (row['contained'] and row['dice'] < 0.6):\n",
    "            continue\n",
    "\n",
    "        key1 = (row['debate_unit_id'], row['label'], row['span_1'])\n",
    "        key2 = (row['debate_unit_id'], row['label'], row['span_2'])\n",
    "\n",
    "        if key1 in gold_spans or key2 in gold_spans:\n",
    "            continue\n",
    "\n",
    "        shared_span = get_longest_common_substring(row['span_1'], row['span_2'])\n",
    "        if shared_span and word_count(shared_span) >= min_words:\n",
    "            output_rows.append({\n",
    "                'debate_unit_id': row['debate_unit_id'],\n",
    "                'label': row['label'].strip(),\n",
    "                'span': shared_span.strip(),\n",
    "                'span_1': row['span_1'],\n",
    "                'span_2': row['span_2'],\n",
    "                #'source_annotators': f\"{row['annotator_1']} and {row['annotator_2']}\",\n",
    "                'source_annotators': f\"{row['annotator_1']},{row['annotator_2']}\",\n",
    "                'construction_note': 'Contained but low Dice (post hoc)',\n",
    "                'dice': row['dice'],\n",
    "                'contained': row['contained']\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(output_rows)\n",
    "\n",
    "    # 🧠 Merge duplicates by span with label consolidation\n",
    "    def merge_labels(labels):\n",
    "        return ' + '.join(sorted(set(labels)))\n",
    "\n",
    "    def merge_annotators(sources):\n",
    "        # Flatten and deduplicate\n",
    "        raw = ','.join(sources)  # Keep it comma-separated\n",
    "        all_annos = sorted(set(re.split(r'[,\\s]+', raw)))\n",
    "        return f\"common substring of {', '.join(all_annos)}\"\n",
    "\n",
    "    df_merged = (\n",
    "        df.groupby(['debate_unit_id', 'span'], as_index=False)\n",
    "          .agg({\n",
    "              'label': merge_labels,\n",
    "              'source_annotators': merge_annotators,\n",
    "              'construction_note': lambda x: '; '.join(sorted(set(x))),\n",
    "              'span_1': 'first',\n",
    "              'span_2': 'first',\n",
    "              'dice': 'first',\n",
    "              'contained': 'first'\n",
    "          })\n",
    "    )\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "# Secondary step to catch dupes very different in length within same debateunitid\n",
    "def collapse_to_shortest_contained_span(df):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    final_rows = []\n",
    "    grouped = df.groupby('debate_unit_id')\n",
    "\n",
    "    for debate_id, group in grouped:\n",
    "        rows = group.to_dict('records')\n",
    "        used = set()\n",
    "        collapsed_spans = []\n",
    "\n",
    "        for i in range(len(rows)):\n",
    "            if i in used:\n",
    "                continue\n",
    "            span_i = rows[i]['span']\n",
    "            annos_i = set(re.split(r'[,\\s]+', rows[i]['source_annotators'].replace(\"common substring of\", \"\").strip()))\n",
    "            label_i = set(rows[i]['label'].split(' + '))\n",
    "            note_i = rows[i]['construction_note']\n",
    "\n",
    "            # Look for contained spans\n",
    "            for j in range(i + 1, len(rows)):\n",
    "                if j in used:\n",
    "                    continue\n",
    "                span_j = rows[j]['span']\n",
    "\n",
    "                if span_i in span_j or span_j in span_i:\n",
    "                    # Use the shorter one\n",
    "                    shorter_span = span_i if len(span_i) <= len(span_j) else span_j\n",
    "                    span_i = shorter_span\n",
    "\n",
    "                    # Merge annotators and labels\n",
    "                    annos_j = set(re.split(r'[,\\s]+', rows[j]['source_annotators'].replace(\"common substring of\", \"\").strip()))\n",
    "                    annos_i.update(annos_j)\n",
    "\n",
    "                    label_j = set(rows[j]['label'].split(' + '))\n",
    "                    label_i.update(label_j)\n",
    "\n",
    "                    used.add(j)\n",
    "\n",
    "            final_rows.append({\n",
    "                'debate_unit_id': debate_id,\n",
    "                'label': ' + '.join(sorted(label_i)),\n",
    "                'span': span_i.strip(),\n",
    "                'source_annotators': 'common substring of ' + ', '.join(sorted(annos_i)),\n",
    "                'construction_note': 'Reduced to shortest span after containment check'\n",
    "            })\n",
    "\n",
    "            used.add(i)\n",
    "\n",
    "    return pd.DataFrame(final_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary check\n",
    "containment_posthoc_df = find_containment_only_matches_with_label_merge(matches_df, gold_df, min_words=3)\n",
    "\n",
    "# Secondary check\n",
    "collapsed_shortest_df = collapse_to_shortest_contained_span(containment_posthoc_df)\n",
    "collapsed_shortest_df.to_csv(\"output/prelim_dataframes/posthoc_containment_matches.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I do manual review (1 keep span and label, 0 discard) and then we reload in, and then append to gold df\n",
    "import pandas as pd\n",
    "containment_posthos_df_edited = pd.read_csv(\"output/prelim_dataframes/posthoc_containment_matches_manual_edit.csv\", sep=\";\")\n",
    "\n",
    "# Get the gold + tiebreak aug df\n",
    "gold_df_augmented = pd.read_csv('/Users/pbrams/Desktop/AARHUS_UNIVERSITY/kandidat/thesis_work/annotations/output/coalesced_gold_augmented_df.csv')\n",
    "gold_df_augmented_c = gold_df_augmented.copy()\n",
    "\n",
    "gold_df_augmented_c['quality_status'] = gold_df_augmented_c['disagreement'].map({\n",
    "    False: 'pure gold',\n",
    "    True: 'from disagreement'\n",
    "})\n",
    "\n",
    "# Drop the old one\n",
    "gold_df_augmented_c = gold_df_augmented_c.drop(columns='disagreement') # 439 rows\n",
    "\n",
    "# Now merge\n",
    "# Filter and tag\n",
    "to_add = containment_posthos_df_edited[containment_posthos_df_edited['keep_flag'] == 1].copy()\n",
    "to_add['quality_status'] = 'result of low dice but containment check'\n",
    "\n",
    "# Add missing columns to match\n",
    "if 'disagreement' in to_add.columns:\n",
    "    to_add = to_add.drop(columns='disagreement')\n",
    "if 'keep_flag' in to_add.columns:\n",
    "    to_add = to_add.drop(columns='keep_flag')\n",
    "\n",
    "# Make sure both dataframes have same columns\n",
    "common_cols = list(set(gold_df_augmented_c.columns) & set(to_add.columns))\n",
    "gold_df_augmented_c = gold_df_augmented_c[common_cols]\n",
    "to_add = to_add[common_cols]\n",
    "\n",
    "# Append\n",
    "gold_final_aug_df = pd.concat([gold_df_augmented_c, to_add], ignore_index=True)\n",
    "\n",
    "# Reorder\n",
    "ordered_cols = [\n",
    "    'debate_unit_id',\n",
    "    'label',\n",
    "    'span',\n",
    "    'quality_status',\n",
    "    'source_annotators',\n",
    "    'construction_note'\n",
    "]\n",
    "\n",
    "gold_final_aug_df = gold_final_aug_df[ordered_cols]\n",
    "\n",
    "gold_final_aug_df.to_csv(\"output/coalesced_gold_aug_three_levels_18_april.csv\")\n",
    "gold_final_aug_df # 595 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality_status</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pure gold</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>result of low dice but containment check</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from disagreement</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             quality_status  count\n",
       "0                                 pure gold    369\n",
       "1  result of low dice but containment check    155\n",
       "2                         from disagreement     70"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "counts = gold_final_aug_df['quality_status'].value_counts(dropna=False)\n",
    "\n",
    "counts.rename_axis('quality_status').reset_index(name='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>answer</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evasion</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attack</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>self_promotion</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>evasion + attack</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>evasion + self_promotion</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>evasion + stretch</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>attack + self_promotion</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>attack + stretch</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>self_promotion + stretch</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stretch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       label  count\n",
       "0                     answer    267\n",
       "1                    evasion    169\n",
       "2                     attack     54\n",
       "3             self_promotion     33\n",
       "4           evasion + attack     25\n",
       "5   evasion + self_promotion     23\n",
       "6          evasion + stretch     11\n",
       "7    attack + self_promotion      4\n",
       "8           attack + stretch      4\n",
       "9   self_promotion + stretch      3\n",
       "10                   stretch      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = gold_final_aug_df['label'].value_counts(dropna=False)\n",
    "label_counts_df = label_counts.rename_axis('label').reset_index(name='count')\n",
    "label_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evasion + attack</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evasion + self_promotion</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>evasion + stretch</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attack + self_promotion</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>attack + stretch</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>self_promotion + stretch</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label  count\n",
       "0          evasion + attack     25\n",
       "1  evasion + self_promotion     23\n",
       "2         evasion + stretch     11\n",
       "3   attack + self_promotion      4\n",
       "4          attack + stretch      4\n",
       "5  self_promotion + stretch      3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows where quality_status == 'from disagreement'\n",
    "filtered_df = gold_final_aug_df_check[gold_final_aug_df_check['quality_status'] == 'from disagreement']\n",
    "\n",
    "# Breakdown of row counts by 'label'\n",
    "label_counts = filtered_df['label'].value_counts(dropna=False)\n",
    "label_counts_df = label_counts.rename_axis('label').reset_index(name='count')\n",
    "label_counts_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
